---
title: "Optimization and related uses of autodiffr: Illustrations"
author: "Changcheng Li and John C. Nash"
date: "2018/7/1"
output: pdf_document
bibliography: opt.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

`autodiffr` is an **R** package to perform automatic differentiation of
**R** functions by calling the automatic differentiation tools in the 
**Julia** programming language. The document **FandR** (??ref) describes
how to install `autodiffr`.

Here we will illustrate how `autodiffr` can be used to provide gradients
or other derivative information for use in optimization problems for which
**R** is being used to attempt solutions.

## Problem setup

Most methods for optimization require

 - an objective function that is to be minimized or maximized. Because the 
 mimimum of $f(x)$ is the maximum of $-f(x)$, we will only talk of 
 minimizing functions. Though the mathematics of this are trivial, the
 care and attention to avoid errors when translating a maximization 
 problem to one involving minimization require serious effort and 
 continuous checking.
 - a starting set of values for the parameters to be optimized (and
 perhaps also for any exogenous data and/or fixed parameters)


```{r}
library(autodiffr)
ad_setup()
```
And we can use package `numDeriv` to compare with `autodiffr`.

```{r}
require(numDeriv)
```


## Test problem -- ViaRes

This is simply to test how we can get the gradient of a function that is defined as the 
sum of squares of residuals, BUT the residuals are computed in a subsidiary function 
that must be called.

```{r}
require(autodiffr)
ad_setup() # to ensure it is established

ores <- function(x){
    x # Function will be the parameters. ofn is sum of squares
}

ofn <- function(x){
   res <- ores(x) # returns a vector of residual values
   print(res)
   val <- as.numeric(crossprod(res)) # as.numeric because crossprod is a matrix
   val
}

## Now try to generate the gradient function
ogr <- grad(ofn)
print(ogr) # this will be more or less meaningless link to Julia function
x0 <- c(1,2,3)
print(ofn(x0)) # should be 14
print(ogr(x0)) # should be 2, 4, 6
```


## Test problem -- Chebyquad

This problem was given prominence in the optimization literature by @Fletcher65.

Here are some extra pieces of JN's cyq.R.

```
# Ref: Fletcher, R. (1965) Function minimization without calculating derivatives -- a review,
#         Computer J., 8, 33-41.

# Note we do not have all components here e.g., .jsd, .h

cyq.jac<- function (x) {
#  Chebyquad Jacobian matrix
   n<-length(x)
   cj<-matrix(0.0, n, n)
   for (i in 1:n) { # loop over rows
     for (k in 1:n) { # loop over columns (parameters)
       z5<-0.0
       cj[i,k]<-2.0
       z8<-2.0*x[k]-1.0 
       z2<-z8
       z7<-1.0
       j<- 1
       while (j<i) { # recurrence loop
         z4<-z5
         z5<-cj[i,k]
         cj[i,k]<-4.0*z8+2.0*z2*z5-z4
         z6<-z7
         z7<-z8
         z8<-2.0*z2*z7-z6
         j<- j+1
       } # end recurrence loop
       cj[i,k]<-cj[i,k]/n
     } # end loop on k
   } # end loop on i
   cj
}


cyq.g <- function (x) {
   cj<-cyq.jac(x)
   rv<-cyq.res(x)
   gg<- as.vector(2.0* rv %*% cj)
}


```

First let us define our Chebyquad function. Note that this is for the **vector** x.
This version is expressed as the sum of squares of a vector of function values,
which provide a nonlinear least squares problem.

```{r}
cyq.f <- function (x) {
  rv<-cyq.res(x)
  f<-sum(rv*rv)
}

cyq.res <- function (x) {
# Fletcher's chebyquad function m = n -- residuals 
   n<-length(x)
   res<-rep(0,n) # initialize
   for (i in 1:n) { #loop over resids
     rr<-0.0
     for (k in 1:n) {
	z7<-1.0
	z2<-2.0*x[k]-1.0
        z8<-z2
        j<-1
        while (j<i) {
            z6<-z7
            z7<-z8
            z8<-2*z2*z7-z6 # recurrence to compute Chebyshev polynomial
            j<-j+1
        } # end recurrence loop
        rr<-rr+z8
      } # end loop on k
      rr<-rr/n
      if (2*trunc(i/2) == i) { rr <- rr + 1.0/(i*i - 1) }
      res[i]<-rr
    } # end loop on i
    res
}
```

Let us choose a single value for the number of parameters, and for illustration
use $n = 4$. 

```{r}
## cyq.setup
n <- 4
  lower<-rep(-10.0, n)
  upper<-rep(10.0, n) 
  x<-1:n
  x<-x/(n+1.0) # Initial value suggested by Fletcher
```

For safety, let us check the function and a numerical approximation to the gradient.

```{r}
require(numDeriv)
cat("Initial parameters:")
print(x)
cat("Initial value of the function is ",cyq.f(x),"\n")
gn <- numDeriv::grad(cyq.f, x) # using numDeriv
cat("Approximation to gradient at initial point:")
print(gn)
```


We can now try to see if `autodiffr` matches this gradient.


We can get the Chebyquad function from the package `funconstrain`
as well as from JN's function collection. Moreover, the `funconstrain`
offering does NOT require a call to the residuals, but has a single
level **R** function.

```{r, eval=TRUE, echo=TRUE}
require(funconstrain)
cat("funconstrain loaded\n")
cheb <- chebyquad() # Seem to need the brackets or doesn't return pieces
print(str(cheb))
cyq2.f <- cheb$fn
x0b <- cheb$x0(n=4) # Need the size of the vector
x0b
cyq2.f(x0b)
```
Split chunks to narrow down error

```{r}
## Try the gradient
cyq2.g <- autodiffr::grad(cyq2.f) # Works in console, NOT in knit without autodiffr specified ??
```

Isolate the troublesome line.

```{r}
print(cyq2.g)
cat("Gradient at x0b")
print(cyq2.g(x0b))
```



```{r, eval=FALSE}
cyq.ag <- autodiffr::grad(cyq.f,x) # should return a function
## Seems to hang -- doesn't execute, or does it?
cat("Result of grad(cyq.f):\n")
```

We isolate the line to see where failure is located.

```{r, eval=FALSE}
print(cyq.ag)
cat("Evaluated at x0:")
print(cyq.ag(x))
```

Chebyquad Questions: 

 - How fast is the calculation? Try different ways and produce a table.
 
 - Why is JN's cyq.ag NOT working? Does it have something to do with calling cyq.res to compute cyq.f?
 
## Test problem -- Hobbs weed infestation


## Test problem -- Wood


## Test problem -- Candlestick

```{r}
# candlestick function
# J C Nash 2011-2-3
cstick.f<-function(x,alpha=1){
	x<-as.vector(x)
	r2<-crossprod(x)
	f<-as.double(r2+alpha/r2)
	return(f)
}

cstick.g<-function(x,alpha=1){
	x<-as.vector(x)
	r2<-crossprod(x)
	g1<-2*x
	g2 <- (-alpha)*2*x/(r2*r2)
	g<-as.double(g1+g2)
	return(g)
}
```


## Wood 4 parameter function

```{r}
require(autodiffr)
ad_setup() # to ensure it is established
#Example 2: Wood function
#
wood.f <- function(x){
  res <- 100*(x[1]^2-x[2])^2+(1-x[1])^2+90*(x[3]^2-x[4])^2+(1-x[3])^2+
    10.1*((1-x[2])^2+(1-x[4])^2)+19.8*(1-x[2])*(1-x[4])
  return(res)
}
#gradient:
wood.g <- function(x){
  g1 <- 400*x[1]^3-400*x[1]*x[2]+2*x[1]-2
  g2 <- -200*x[1]^2+220.2*x[2]+19.8*x[4]-40
  g3 <- 360*x[3]^3-360*x[3]*x[4]+2*x[3]-2
  g4 <- -180*x[3]^2+200.2*x[4]+19.8*x[2]-40
  return(c(g1,g2,g3,g4))
}
#hessian:
wood.h <- function(x){
  h11 <- 1200*x[1]^2-400*x[2]+2;    h12 <- -400*x[1]; h13 <- h14 <- 0
  h22 <- 220.2; h23 <- 0;    h24 <- 19.8
  h33 <- 1080*x[3]^2-360*x[4]+2;    h34 <- -360*x[3]
  h44 <- 200.2
  H <- matrix(c(h11,h12,h13,h14,h12,h22,h23,h24,
                h13,h23,h33,h34,h14,h24,h34,h44),ncol=4)
  return(H)
}
#################################################
x0 <- c(-3,-1,-3,-1) # Wood standard start

cat("Function value at x0=",wood.f(x0),"\n")
wood.ag <- autodiffr::grad(wood.f)
cat("Autodiffr gradient value:")
print(wood.ag(x0))
cat("Manually coded:")
print(wood.g(x0))



cat("Autodiffr hessian of function value:")
wood.ah <- autodiffr::hessian(wood.f)
print(wood.ah(x0))
cat("Autodiffr hessian via jacobian of autodiff gradient value:")
wood.ahjag <- autodiffr::jacobian(wood.ag)
print(wood.ah(x0))

cat("Autodiffr hessian via jacobian of manual gradient value:")
wood.ahj <- autodiffr::jacobian(wood.g)
print(wood.ah(x0))
cat("Manually coded:")
print(wood.h(x0))

## d <- c(1,1,1,1)
require(optimx)
wdefault <- snewton(x0, fn=wood.f, gr=wood.g, hess=wood.h, control=list(trace=2))
print(wdefault)

wagah <- snewton(x0, fn=wood.f, gr=wood.ag, hess=wood.ah, control=list(trace=2))
print(wagah)


```
## Bibliography

