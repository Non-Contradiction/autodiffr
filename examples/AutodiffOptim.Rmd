---
title: "Optimization and related uses of autodiffr: Illustrations"
author: "Changcheng Li and John C. Nash"
date: "2018/7/1"
output: pdf_document
bibliography: opt.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

`autodiffr` is an **R** package to perform automatic differentiation of
**R** functions by calling the automatic differentiation tools in the 
**Julia** programming language. The document **FandR** (??ref) describes
how to install `autodiffr`.

Here we will illustrate how `autodiffr` can be used to provide gradients
or other derivative information for use in optimization problems for which
**R** is being used to attempt solutions.

## Problem setup

Most methods for optimization require

 - an objective function that is to be minimized or maximized. Because the 
 mimimum of $f(x)$ is the maximum of $-f(x)$, we will only talk of 
 minimizing functions. Though the mathematics of this are trivial, the
 care and attention to avoid errors when translating a maximization 
 problem to one involving minimization require serious effort and 
 continuous checking.
 - a starting set of values for the parameters to be optimized (and
 perhaps also for any exogenous data and/or fixed parameters)


We need to load the `autodiffr` package and then initiate it.
NOTE: This is quite slow the first time it is run. WORSE: It is
always treated as a "first time" when called in knitr during the
processing of a vignette from an Rmd-type file.

```{r}
library(autodiffr)
ad_setup()
```
And we can use package `numDeriv` to compare with `autodiffr`.

```{r}
require(numDeriv)
```


## Test problem -- ViaRes

This is simply to test how we can get the gradient of a function that is defined as the 
sum of squares of residuals, BUT the residuals are computed in a subsidiary function 
that must be called.

At July 2, 2018, this gives an error that stops knitr, so evaluation is turned off
in the following example.

```{r}
require(autodiffr)
ad_setup() # to ensure it is established

ores <- function(x){
    x # Function will be the parameters. ofn is sum of squares
}

ofn0 <- function(x){ # original ofn
   res <- ores(x) # returns a vector of residual values
   print(res)
   val <- as.numeric(crossprod(res)) # as.numeric because crossprod is a matrix
   val
}

ofn <- function(x){ # But autodiffr does not understand crossprod()
   res <- ores(x) # returns a vector of residual values
   # print(res)
   val <- sum(res*res) # as.numeric because crossprod is a matrix
   val
}
```

```{r, eval=FALSE}
## Now try to generate the gradient function
ogr <- grad(ofn)

# print(ogr) # this will be more or less meaningless link to Julia function
x0 <- c(1,2,3)
print(ofn(x0)) # should be 14
print(ofn0(x0)) # should be 14
ogr0<-ogr(x0) # should be 2, 4, 6
ogr0
```


## Test problem -- Chebyquad

This problem was given prominence in the optimization literature by @Fletcher65.

Here are some extra pieces of JN's cyq.R.

```
# Ref: Fletcher, R. (1965) Function minimization without calculating derivatives -- a review,
#         Computer J., 8, 33-41.

# Note we do not have all components here e.g., .jsd, .h

cyq.jac<- function (x) {
#  Chebyquad Jacobian matrix
   n<-length(x)
   cj<-matrix(0.0, n, n)
   for (i in 1:n) { # loop over rows
     for (k in 1:n) { # loop over columns (parameters)
       z5<-0.0
       cj[i,k]<-2.0
       z8<-2.0*x[k]-1.0 
       z2<-z8
       z7<-1.0
       j<- 1
       while (j<i) { # recurrence loop
         z4<-z5
         z5<-cj[i,k]
         cj[i,k]<-4.0*z8+2.0*z2*z5-z4
         z6<-z7
         z7<-z8
         z8<-2.0*z2*z7-z6
         j<- j+1
       } # end recurrence loop
       cj[i,k]<-cj[i,k]/n
     } # end loop on k
   } # end loop on i
   cj
}


cyq.g <- function (x) {
   cj<-cyq.jac(x)
   rv<-cyq.res(x)
   gg<- as.vector(2.0* rv %*% cj)
}


```

First let us define our Chebyquad function. Note that this is for the **vector** x.
This version is expressed as the sum of squares of a vector of function values,
which provide a nonlinear least squares problem.

```{r}
cyq.f <- function (x) {
  rv<-cyq.res(x)
  f<-sum(rv*rv)
}

cyq.res <- function (x) {
# Fletcher's chebyquad function m = n -- residuals 
   n<-length(x)
   res<-rep(0,n) # initialize
   for (i in 1:n) { #loop over resids
     rr<-0.0
     for (k in 1:n) {
	z7<-1.0
	z2<-2.0*x[k]-1.0
        z8<-z2
        j<-1
        while (j<i) {
            z6<-z7
            z7<-z8
            z8<-2*z2*z7-z6 # recurrence to compute Chebyshev polynomial
            j<-j+1
        } # end recurrence loop
        rr<-rr+z8
      } # end loop on k
      rr<-rr/n
      if (2*trunc(i/2) == i) { rr <- rr + 1.0/(i*i - 1) }
      res[i]<-rr
    } # end loop on i
    res
}
```

Let us choose a single value for the number of parameters, and for illustration
use $n = 4$. 

```{r}
## cyq.setup
n <- 4
  lower<-rep(-10.0, n)
  upper<-rep(10.0, n) 
  x<-1:n
  x<-x/(n+1.0) # Initial value suggested by Fletcher
```

For safety, let us check the function and a numerical approximation to the gradient.

```{r}
require(numDeriv)
cat("Initial parameters:")
print(x)
cat("Initial value of the function is ",cyq.f(x),"\n")
gn <- numDeriv::grad(cyq.f, x) # using numDeriv
cat("Approximation to gradient at initial point:")
print(gn)
```


We can now try to see if `autodiffr` matches this gradient.


We can get the Chebyquad function from the package `funconstrain`
as well as from JN's function collection. Moreover, the `funconstrain`
offering does NOT require a call to the residuals, but has a single
level **R** function.

```{r, eval=TRUE, echo=TRUE}
require(funconstrain)
cat("funconstrain loaded\n")
cheb <- chebyquad() # Seem to need the brackets or doesn't return pieces
print(str(cheb))
cyq2.f <- cheb$fn
x0b <- cheb$x0(n=4) # Need the size of the vector
x0b
cyq2.f(x0b)
```
Split chunks to narrow down error

```{r}
## Try the gradient
cyq2.g <- autodiffr::grad(cyq2.f) # Works in console, NOT in knit without autodiffr specified ??
```

Isolate the troublesome line.

```{r}
print(cyq2.g)
cat("Gradient at x0b")
print(cyq2.g(x0b))
```



```{r, eval=FALSE}
cyq.ag <- autodiffr::grad(cyq.f,x) # should return a function
## Seems to hang -- doesn't execute, or does it?
cat("Result of grad(cyq.f):\n")
```

We isolate the line to see where failure is located.

```{r, eval=FALSE}
print(cyq.ag)
cat("Evaluated at x0:")
print(cyq.ag(x))
```

Chebyquad Questions: 

 - How fast is the calculation? Try different ways and produce a table.
 
 - Why is JN's cyq.ag NOT working? Does it have something to do with calling cyq.res to compute cyq.f?
   NO. crossprod()
 
## Test problem -- Hobbs weed infestation




## Test problem -- Candlestick

This function was developed by one of us to provide a simple but (for n equal 1 or 2) graphic example of a
function with an infinity of solutions. The function can be seen by graphing it to have 
a 

```{r}
# candlestick function
# J C Nash 2011-2-3
cstick.f<-function(x,alpha=1){
	x<-as.vector(x)
	r2<-sum(x*x)
	f<-as.double(r2+alpha/r2)
	return(f)
}

cstick.g<-function(x,alpha=1){
	x<-as.vector(x)
	r2<-sum(x*x)
	g1<-2*x
	g2 <- (-alpha)*2*x/(r2*r2)
	g<-as.double(g1+g2)
	return(g)
}

x <- seq(-100:100)/20.0
y <- x

for (ii in 1:length(x)){
    y[ii] <- cstick.f(x[ii])
}
plot(x, y, type='l') # ?? does not plot from console??
x0 <- c(1,2)
require(optimx)
sdef0 <- optimr(x0, cstick.f, cstick.g, method="Rvmmin", control=list(trace=1))
sdef0
xstar <- sdef0$par
gstar <- cstick.g(xstar)
cat("Gradient at proposed solution:")
print(gstar)
```

```{r, eval=FALSE}
## This doesn't seem to work well??
require(autodiffr)
ad_setup()
hc <- hessian(cstick.f)
hstar<-hc(xstar)
cat("Hessian at proposed solution:\n")
print(hstar)
print(eigen(hstar)$values)
hc(x0)
```


## ## Test problem -- Wood 4 parameter function

This is reported by @more81 as coming from @colville68. The problem in 
4 parameters seems to have a false solution far from the accepted one.
Is there a good description of this function and the issues it presents?

```{r}
require(autodiffr)
ad_setup() # to ensure it is established
#Example 2: Wood function
#
wood.f <- function(x){
  res <- 100*(x[1]^2-x[2])^2+(1-x[1])^2+90*(x[3]^2-x[4])^2+(1-x[3])^2+
    10.1*((1-x[2])^2+(1-x[4])^2)+19.8*(1-x[2])*(1-x[4])
  return(res)
}
#gradient:
wood.g <- function(x){
  g1 <- 400*x[1]^3-400*x[1]*x[2]+2*x[1]-2
  g2 <- -200*x[1]^2+220.2*x[2]+19.8*x[4]-40
  g3 <- 360*x[3]^3-360*x[3]*x[4]+2*x[3]-2
  g4 <- -180*x[3]^2+200.2*x[4]+19.8*x[2]-40
  return(c(g1,g2,g3,g4))
}
#hessian:
wood.h <- function(x){
  h11 <- 1200*x[1]^2-400*x[2]+2;    h12 <- -400*x[1]; h13 <- h14 <- 0
  h22 <- 220.2; h23 <- 0;    h24 <- 19.8
  h33 <- 1080*x[3]^2-360*x[4]+2;    h34 <- -360*x[3]
  h44 <- 200.2
  H <- matrix(c(h11,h12,h13,h14,h12,h22,h23,h24,
                h13,h23,h33,h34,h14,h24,h34,h44),ncol=4)
  return(H)
}
#################################################
x0 <- c(-3,-1,-3,-1) # Wood standard start

cat("Function value at x0=",wood.f(x0),"\n")
wood.ag <- autodiffr::grad(wood.f)
cat("Autodiffr gradient value:")
vwag0<-wood.ag(x0)
print(vwag0)
cat("Manually coded:")
vwg0 <- wood.g(x0)
print(vwg0)
cat("Differences:\n")
print(vwag0-vwg0)


cat("Autodiffr hessian of function value:")
wood.ah <- autodiffr::hessian(wood.f)
vwah0 <- wood.ah(x0)
print(vwah0)
cat("Autodiffr hessian via jacobian of autodiff gradient value:")
wood.ahjag <- autodiffr::jacobian(wood.ag)
vwahjag0<-wood.ahjag(x0)
print(vwahjag0)

cat("Autodiffr hessian via jacobian of manual gradient value:")
wood.ahj <- autodiffr::jacobian(wood.g)
vwahj0 <- wood.ah(x0)
print(vwahj0)
cat("Manually coded:")
vwh0<-wood.h(x0)
print(vwh0)
cat("Differences from vwh0\n")

cat("vwah0\n")
print(vwah0-vwh0)
cat("\n")

cat("vwahj0\n")
print(vwahj0-vwh0)
cat("\n")

cat("vwahjag0\n")
print(vwahjag0-vwh0)
cat("\n")

## d <- c(1,1,1,1)
require(optimx)
wdefault <- snewton(x0, fn=wood.f, gr=wood.g, hess=wood.h, control=list(trace=1))
print(wdefault)

wagah <- snewton(x0, fn=wood.f, gr=wood.ag, hess=wood.ah, control=list(trace=1))
print(wagah)
```


## Performance issues

Optimization is, by its very nature, about improving things. Thus it is of prime interest
to seek faster and better ways to optimize functions. In this section we look at some issues
that may influence the speed, reliability and correctness of optimization calculations.

First, it is critical to note that **R** almost always offers several ways to accomplish the
same computational result. However, the speed with which the different approaches return a
result can be wildly different. (?? can JN find the 800% scale factor example??). 

Second, there are many parts of the autodiffr wrapper of Julia's automatic differentiation
that may use up computing cycles:

- We must translate from one programming language to another in some sense in order to 
call the appropriate functions in Julia based on **R** functions.

- Results must be properly structured on return to **R**.

- Hand coded derivative expressions, especially hand-optimized ones, can be expected to
out-perform automatic differentiation results.

NOTE: Performance is interesting, but it is far from the complete picture. We can use
results from autodiffr to validate hand-coded functions. We can get results that are
efficient of human time and effort that may be otherwise unavailable. Moreover, the
results of computing gradients and hessians allow us to conclude that a solution has
been achieved.

### A small performance comparison using `autodiffr`

```{r}
rm(list=ls())
require(autodiffr)
autodiffr::ad_setup() # to ensure it is established

ores <- function(x){
    x # Function will be the parameters. ofn is sum of squares
}

logit <- function(x) exp(x) / (1 + exp(x))

ofn <- function(x){
    res <- ores(x) # returns a vector of residual values
    sum(logit(res) ^ 2)
}

## Now try to generate the gradient function
ogr <- autodiffr::grad(ofn)

system.time(ogr(runif(100)))

system.time(ogr(runif(100)))

ogr1 <- autodiffr::grad(ofn, xsize = runif(100))

system.time(ogr1(runif(100)))

system.time(ogr1(runif(100)))

ogr2 <- autodiffr::grad(ofn, xsize = runif(100), use_tape = TRUE)

system.time(ogr2(runif(100)))

system.time(ogr2(runif(100)))
```






## Bibliography

