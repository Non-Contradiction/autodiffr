% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/reverse.R
\name{ReverseDiff}
\alias{ReverseDiff}
\alias{reverse.grad}
\alias{reverse.jacobian}
\alias{reverse.hessian}
\alias{reverse.grad.config}
\alias{reverse.jacobian.config}
\alias{reverse.hessian.config}
\alias{reverse.grad.tape}
\alias{reverse.jacobian.tape}
\alias{reverse.hessian.tape}
\alias{reverse.compile}
\title{Wrapper functions for API of \code{ReverseDiff.jl}.}
\usage{
reverse.grad(f_or_tape, input, cfg = NULL)

reverse.jacobian(f_or_tape, input, cfg = NULL)

reverse.hessian(f_or_tape, input, cfg = NULL)

reverse.grad.config(input)

reverse.jacobian.config(input)

reverse.hessian.config(input)

reverse.grad.tape(f, input, cfg = reverse.grad.config(input))

reverse.jacobian.tape(f, input, cfg = reverse.jacobian.config(input))

reverse.hessian.tape(f, input, cfg = reverse.hessian.config(input))

reverse.compile(tape)
}
\arguments{
\item{f_or_tape}{the target function \code{f} or the tape recording execution trace of \code{f}.}

\item{input}{the point where you take the gradient, jacobian and hessian.
Note that it should be a a vector of length greater than 1.
If you want to calulate the derivative of a function, you can considering using \code{forward.deriv}.}

\item{cfg}{Config objects which contains the preallocated tape and work buffers
used by reverse mode automatic differentiation.
\code{ReverseDiff}'s API methods will allocate the Config object automatically by default,
but you can preallocate them yourself and reuse them for subsequent calls to reduce memory usage.}

\item{f}{the function you want to calulate the gradient, jacobian and hessian.
Note that \code{f(x)} should be a scalar for \code{grad} and \code{hessian},
a vector of length greater than 1 for \code{jacobian}.}

\item{tape}{the object to record the target function's execution trace used by
reverse mode automatic differentiation.
In many cases, pre-recording and pre-compiling a reusable tape for a given function and
differentiation operation can improve the performance of reverse mode automatic differentiation.
Note that pre-recording a tape can only capture the the execution trace of the target function
with the given input values.
In other words, the tape cannot any re-enact branching behavior that depends on the input values.
If the target functions contain control flow based on the input values, be careful or not to
use tape-related APIs.}
}
\value{
\code{reverse.grad}, \code{reverse.jacobian} and \code{reverse.hessian} return
the gradient, jacobian and hessian of \code{f} or \code{tape} correspondingly evaluated at \code{input}.
\code{reverse.grad.config}, \code{reverse.jacobian.config} and \code{reverse.hessian.config}
return Config instances containing the preallocated tape and work buffers used by
reverse mode automatic differentiation.
\code{reverse.grad.tape}, \code{reverse.jacobian.tape} and \code{reverse.hessian.tape}
return Tape instances containing the the execution trace of the target function
with the given input values.
}
\description{
Wrapper functions for API of \code{ReverseDiff.jl} at
\url{http://www.juliadiff.org/ReverseDiff.jl/api/}.
These functions can help you calculate gradient, jacobian and hessian
for your functions using reverse mode automatic differentiation.
For more details, see \url{http://www.juliadiff.org/ReverseDiff.jl/api/}.
}
